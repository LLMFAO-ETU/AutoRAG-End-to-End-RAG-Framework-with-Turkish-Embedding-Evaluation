{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c2ea0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 1)) (2.5.1+cu118)\n",
      "Requirement already satisfied: transformers>=4.38.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 2)) (4.48.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 3)) (1.7.0)\n",
      "Requirement already satisfied: bitsandbytes>=0.41.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 4)) (0.46.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.2.2 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 5)) (3.3.1)\n",
      "Requirement already satisfied: qdrant-client>=1.6.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 6)) (1.12.2)\n",
      "Requirement already satisfied: pypdf>=3.0.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 7)) (5.6.0)\n",
      "Requirement already satisfied: python-docx>=0.8.11 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 8)) (1.1.2)\n",
      "Requirement already satisfied: nltk>=3.8.1 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 9)) (3.9.1)\n",
      "Requirement already satisfied: unstructured>=0.7.6 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 10)) (0.17.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.1 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 11)) (0.26.2)\n",
      "Collecting bitsandbytes-cuda117 (from -r requirements.txt (line 12))\n",
      "  Downloading bitsandbytes_cuda117-0.26.0.post2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (4.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.38.0->-r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.38.0->-r requirements.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.38.0->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.38.0->-r requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.38.0->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.38.0->-r requirements.txt (line 2)) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.38.0->-r requirements.txt (line 2)) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.38.0->-r requirements.txt (line 2)) (4.65.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate>=0.26.0->-r requirements.txt (line 3)) (5.9.8)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->-r requirements.txt (line 5)) (1.5.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->-r requirements.txt (line 5)) (1.12.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->-r requirements.txt (line 5)) (11.1.0)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qdrant-client>=1.6.0->-r requirements.txt (line 6)) (1.72.1)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qdrant-client>=1.6.0->-r requirements.txt (line 6)) (1.69.0)\n",
      "Requirement already satisfied: httpx>=0.20.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client>=1.6.0->-r requirements.txt (line 6)) (0.28.1)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qdrant-client>=1.6.0->-r requirements.txt (line 6)) (2.10.1)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qdrant-client>=1.6.0->-r requirements.txt (line 6)) (2.11.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qdrant-client>=1.6.0->-r requirements.txt (line 6)) (2.3.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-docx>=0.8.11->-r requirements.txt (line 8)) (5.3.1)\n",
      "Requirement already satisfied: click in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (1.4.2)\n",
      "Requirement already satisfied: chardet in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from unstructured>=0.7.6->-r requirements.txt (line 10)) (5.2.0)\n",
      "Requirement already satisfied: filetype in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from unstructured>=0.7.6->-r requirements.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: python-magic in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from unstructured>=0.7.6->-r requirements.txt (line 10)) (0.4.27)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from unstructured>=0.7.6->-r requirements.txt (line 10)) (4.12.3)\n",
      "Requirement already satisfied: emoji in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from unstructured>=0.7.6->-r requirements.txt (line 10)) (2.14.1)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from unstructured>=0.7.6->-r requirements.txt (line 10)) (0.5.14)\n",
      "Requirement already satisfied: python-iso639 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from unstructured>=0.7.6->-r requirements.txt (line 10)) (2025.2.18)\n",
      "Requirement already satisfied: langdetect in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from unstructured>=0.7.6->-r requirements.txt (line 10)) (1.0.9)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from unstructured>=0.7.6->-r requirements.txt (line 10)) (3.13.0)\n",
      "Requirement already satisfied: backoff in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from unstructured>=0.7.6->-r requirements.txt (line 10)) (2.2.1)\n",
      "Requirement already satisfied: unstructured-client in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from unstructured>=0.7.6->-r requirements.txt (line 10)) (0.36.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from unstructured>=0.7.6->-r requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: python-oxmsg in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from unstructured>=0.7.6->-r requirements.txt (line 10)) (0.0.2)\n",
      "Requirement already satisfied: html5lib in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from unstructured>=0.7.6->-r requirements.txt (line 10)) (1.1)\n",
      "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client>=1.6.0->-r requirements.txt (line 6)) (5.29.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client>=1.6.0->-r requirements.txt (line 6)) (60.2.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client>=1.6.0->-r requirements.txt (line 6)) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client>=1.6.0->-r requirements.txt (line 6)) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client>=1.6.0->-r requirements.txt (line 6)) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client>=1.6.0->-r requirements.txt (line 6)) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client>=1.6.0->-r requirements.txt (line 6)) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client>=1.6.0->-r requirements.txt (line 6)) (4.1.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client>=1.6.0->-r requirements.txt (line 6)) (310)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.10.8->qdrant-client>=1.6.0->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.10.8->qdrant-client>=1.6.0->-r requirements.txt (line 6)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.10.8->qdrant-client>=1.6.0->-r requirements.txt (line 6)) (0.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers>=4.38.0->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->unstructured>=0.7.6->-r requirements.txt (line 10)) (2.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json->unstructured>=0.7.6->-r requirements.txt (line 10)) (3.25.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json->unstructured>=0.7.6->-r requirements.txt (line 10)) (0.9.0)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from html5lib->unstructured>=0.7.6->-r requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from html5lib->unstructured>=0.7.6->-r requirements.txt (line 10)) (0.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: olefile in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-oxmsg->unstructured>=0.7.6->-r requirements.txt (line 10)) (0.47)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers>=4.38.0->-r requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.2.2->-r requirements.txt (line 5)) (3.5.0)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from unstructured-client->unstructured>=0.7.6->-r requirements.txt (line 10)) (24.1.0)\n",
      "Requirement already satisfied: cryptography>=3.1 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from unstructured-client->unstructured>=0.7.6->-r requirements.txt (line 10)) (44.0.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from unstructured-client->unstructured>=0.7.6->-r requirements.txt (line 10)) (1.6.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from unstructured-client->unstructured>=0.7.6->-r requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cryptography>=3.1->unstructured-client->unstructured>=0.7.6->-r requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.6.0->-r requirements.txt (line 6)) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.6.0->-r requirements.txt (line 6)) (4.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured>=0.7.6->-r requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client>=1.6.0->-r requirements.txt (line 6)) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client>=1.6.0->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\memo_\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured>=0.7.6->-r requirements.txt (line 10)) (2.22)\n",
      "Downloading bitsandbytes_cuda117-0.26.0.post2-py3-none-any.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/4.3 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 2.1/4.3 MB 10.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.1/4.3 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 6.5 MB/s eta 0:00:00\n",
      "Installing collected packages: bitsandbytes-cuda117\n",
      "Successfully installed bitsandbytes-cuda117-0.26.0.post2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cda9adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1607005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\memo_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import glob\n",
    "import docx\n",
    "import nltk\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd0ed73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow'u devre dışı bırakma ayarları yapıldı.\n",
      "USE_TF: 0\n",
      "TRANSFORMERS_NO_TF: 1\n",
      "Tüm kütüphaneler başarıyla import edildi!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Transformers'ın TensorFlow modüllerini yüklemesini tamamen engelliyoruz:\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "print(\"TensorFlow'u devre dışı bırakma ayarları yapıldı.\")\n",
    "print(f\"USE_TF: {os.getenv('USE_TF')}\")\n",
    "print(f\"TRANSFORMERS_NO_TF: {os.getenv('TRANSFORMERS_NO_TF')}\")\n",
    "\n",
    "try:\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from pypdf import PdfReader\n",
    "    from qdrant_client import QdrantClient\n",
    "    from qdrant_client.http.models import Distance, VectorParams\n",
    "    from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "    print(\"Tüm kütüphaneler başarıyla import edildi!\")\n",
    "except Exception as e:\n",
    "    print(f\"Import sırasında bir hata oluştu: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a8ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Doküman Parsers ---\n",
    "def parse_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    return \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "\n",
    "def parse_docx(path):\n",
    "    doc = docx.Document(path)\n",
    "    return \"\\n\".join(p.text for p in doc.paragraphs)\n",
    "\n",
    "def parse_txt(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "# Klasördeki tüm dokümanları oku\n",
    "docs = {}\n",
    "for path in glob.glob(\"dosyayolu\", recursive=True): # dosyayolu yerine gerçek klasör yolunu yazın\n",
    "    if path.endswith(\".pdf\"): docs[path] = parse_pdf(path)\n",
    "    elif path.endswith(\".docx\"): docs[path] = parse_docx(path)\n",
    "    elif path.endswith(\".txt\"): docs[path] = parse_txt(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f930b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam 3 adet chunk metni oluşturuldu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\memo_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Metin Parçalama ---\n",
    "nltk.download(\"punkt\")\n",
    "chunks = [] # Orijinal chunk metinlerini burada saklayacağız\n",
    "metadata_for_qdrant = [] # Qdrant'a yüklenecek metadata\n",
    "\n",
    "global_chunk_id = 0 # Qdrant için benzersiz ve sıralı ID\n",
    "\n",
    "for path, text in docs.items():\n",
    "    if not text or not text.strip():\n",
    "        continue\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    max_tokens = 300\n",
    "    chunk_id_in_file_counter = 0 # Her dosya için chunk ID'si\n",
    "\n",
    "    for i in range(0, len(words), max_tokens):\n",
    "        # Bu, embedding için kullanılacak chunk metni\n",
    "        current_chunk_text = \" \".join(words[i : i + max_tokens])\n",
    "\n",
    "        if not current_chunk_text.strip():\n",
    "            continue\n",
    "\n",
    "        chunks.append(current_chunk_text) # Bu liste hem embedding hem de retrieve için kullanılacak\n",
    "        metadata_for_qdrant.append({\n",
    "            \"source\": path,\n",
    "            \"chunk_id_in_file\": chunk_id_in_file_counter, # Dosya içindeki chunk sırası\n",
    "            \"original_text\": current_chunk_text\n",
    "        })\n",
    "        global_chunk_id += 1\n",
    "        chunk_id_in_file_counter += 1\n",
    "\n",
    "if not chunks:\n",
    "    print(\"Hiç chunk üretilemedi. İşlem durduruluyor.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Toplam {len(chunks)} adet chunk metni oluşturuldu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65b81c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y tensorflow tensorflow-cpu tensorflow-gpu tensorboard tb-nightly tensorflow-estimator tensorflow-io tensorflow-io-gcs-filesystem tf-keras keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6add2e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.72s/it]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#embed_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "embed_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "# Tüm chunk’lar için embedding üret\n",
    "embeddings = embed_model.encode(chunks, show_progress_bar=True, convert_to_numpy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9933e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'llmfao_collection' koleksiyonu silindi.\n",
      "'llmfao_collection' koleksiyonu oluşturuldu/yeniden oluşturuldu.\n",
      "3 nokta başarıyla Qdrant'taki 'llmfao_collection' koleksiyonuna yüklendi.\n"
     ]
    }
   ],
   "source": [
    "# 4. Qdrant Bağlantısı ve İndeksleme \n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "collection_name = \"llmfao_collection\"\n",
    "\n",
    "vector_size = embeddings.shape[1]\n",
    "\n",
    "try:\n",
    "    qdrant_client.delete_collection(collection_name=collection_name)\n",
    "    print(f\"'{collection_name}' koleksiyonu silindi.\")\n",
    "except Exception:\n",
    "    print(f\"'{collection_name}' koleksiyonu silinemedi veya zaten yoktu.\")\n",
    "\n",
    "# Sonra koleksiyonu oluştur\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    ")\n",
    "print(f\"'{collection_name}' koleksiyonu oluşturuldu/yeniden oluşturuldu.\")\n",
    "\n",
    "# Upsert\n",
    "points = [\n",
    "    {\"id\": idx, \n",
    "     \"vector\": embeddings[idx].tolist(), \n",
    "     \"payload\": metadata_for_qdrant[idx]}\n",
    "    for idx in range(len(embeddings))\n",
    "]\n",
    "qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "print(f\"{len(points)} nokta başarıyla Qdrant'taki '{collection_name}' koleksiyonuna yüklendi.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ec33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Retrieve Fonksiyonu (en yakın 5 sonuç) \n",
    "def retrieve_top_k(query: str, k: int = 5):\n",
    "    q_emb = embed_model.encode([query], convert_to_numpy=True)[0]\n",
    "    hits = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=q_emb.tolist(),\n",
    "        limit=k,\n",
    "    )\n",
    "    retrieved_results = []\n",
    "    for hit in hits:\n",
    "        source = hit.payload.get(\"source\", \"Kaynak bilinmiyor\")\n",
    "        chunk_id_val = hit.payload.get(\"chunk_id\", -1) \n",
    "        text_content = hit.payload.get(\"original_text\", \"Metin bulunamadı\")\n",
    "\n",
    "        retrieved_results.append({\n",
    "            \"source\": source,\n",
    "            \"chunk_id\": chunk_id_val,\n",
    "            \"score\": hit.score,\n",
    "            \"text\": text_content,\n",
    "            \"qdrant_id\": hit.id\n",
    "        })\n",
    "    return retrieved_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d912a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "#gpt-2\n",
    "\n",
    "\"\"\"model_id = \"ytu-ce-cosmos/turkish-gpt2-large\"\n",
    "\n",
    "# 1) Tokenizer yükleme\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 2) Model yükleme\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 3) Text-generation pipeline oluşturma\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec25c3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_id = \"ytu-ce-cosmos/Turkish-Gemma-9b-v0.1\"\\n\\nquantization_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_quant_type=\"nf4\",\\n    bnb_4bit_use_double_quant=True,\\n    bnb_4bit_compute_dtype=torch.bfloat16\\n)\\n\\nprint(f\"\\'{model_id}\\' için tokenizer yükleniyor...\")\\n\\ntry:\\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\\n    print(\"Tokenizer başarıyla yüklendi.\")\\n\\n    print(f\"\\'{model_id}\\' modeli BitsAndBytesConfig ile yükleniyor...\")\\n    # torch_dtype\\'ı from_pretrained içinde belirtmek daha doğru olabilir\\n    model = AutoModelForCausalLM.from_pretrained(\\n        model_id,\\n        quantization_config=quantization_config,\\n        torch_dtype=torch.bfloat16, # Modelin ana veri tipi\\n        device_map=\"auto\", # Accelerate\\'in modeli dağıtmasına izin ver\\n        # max_memory parametresini burada da deneyebilirsiniz:\\n        # max_memory={0: \"8GiB\", \"cpu\": \"24GiB\"} # GPU ve CPU limitlerini ayarlayın\\n    )\\n    print(\"Model başarıyla yüklendi.\")\\n\\n    print(\"Pipeline oluşturuluyor...\")\\n    pipeline = transformers.pipeline(\\n        \"text-generation\",\\n        model=model,\\n        tokenizer=tokenizer\\n    )\\nexcept Exception as e:\\n    print(f\"Model yükleme veya pipeline oluşturma sırasında bir hata oluştu: {e}\")}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gemma-9b\n",
    "model_id = \"ytu-ce-cosmos/Turkish-Gemma-9b-v0.1\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(f\"'{model_id}' için tokenizer yükleniyor...\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    print(\"Tokenizer başarıyla yüklendi.\")\n",
    "\n",
    "    print(f\"'{model_id}' modeli BitsAndBytesConfig ile yükleniyor...\")\n",
    "    # torch_dtype'ı from_pretrained içinde belirtmek daha doğru olabilir\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        torch_dtype=torch.bfloat16, # Modelin ana veri tipi\n",
    "        device_map=\"auto\", # Accelerate'in modeli dağıtmasına izin ver\n",
    "        # max_memory parametresini burada da deneyebilirsiniz:\n",
    "        # max_memory={0: \"8GiB\", \"cpu\": \"24GiB\"} # GPU ve CPU limitlerini ayarlayın\n",
    "    )\n",
    "    print(\"Model başarıyla yüklendi.\")\n",
    "\n",
    "    print(\"Pipeline oluşturuluyor...\")\n",
    "    # Değişiklik: 'pipeline' değişkeninin adını 'text_generator' olarak değiştirdik\n",
    "    text_generator = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0\n",
    "    )\n",
    "    print(\"Pipeline başarıyla oluşturuldu.\")\n",
    "except Exception as e:\n",
    "    print(f\"Model yükleme veya pipeline oluşturma sırasında bir hata oluştu: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dccd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gemma 9b için\n",
    "\n",
    "def rag_answer(query: str):\n",
    "    # Retrieve\n",
    "    results = retrieve_top_k(query, k=5)\n",
    "    # Kaynak metinleri oluştur\n",
    "    sources_text = \"\\n\\n\".join(\n",
    "        f\"[{i+1}] Skor: {r['score']:.4f}\\n{r['text']}\"\n",
    "        for i, r in enumerate(results)\n",
    "    )\n",
    "    # Prompt\n",
    "    system_prompt = \"Sen Türkçe NLP uzmanısın. Aşağıdaki kaynaklara dayanarak soruyu yanıtla.\"\n",
    "    user_prompt = f\"Soru: {query}\\n\\nKaynaklar:\\n{sources_text}\\n\\nCevabı açıkla:\"\n",
    "\n",
    "    full_prompt = system_prompt + \"\\n\" + user_prompt\n",
    "\n",
    "    # Cevap üret\n",
    "    outputs = text_generator(\n",
    "        full_prompt,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a685fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpt-2 için\n",
    "\n",
    "#def rag_answer(query: str) -> str:\n",
    "\"\"\"   # 1) Retrieve\n",
    "    results = retrieve_top_k(query, k=5)\n",
    "\n",
    "    # 2) Kaynak metinlerini ve skorları birleştir\n",
    "    sources = [\n",
    "        f\"[{i+1}] Skor: {r['score']:.4f}\\n{r['text']}\"\n",
    "        for i, r in enumerate(results)\n",
    "    ]\n",
    "\n",
    "    # 3) Model ve tokenizer bilgi\n",
    "    max_pos = model.config.max_position_embeddings   # örn. 1024\n",
    "    max_new = 512                                    # üretilecek token\n",
    "    reserved = 50                                    # sistem+format için küçük pay\n",
    "\n",
    "    # 4) Soru + format token ihtiyacı\n",
    "    prefix = (\n",
    "        \"Sen Türkçe NLP uzmanısın. Aşağıdaki kaynaklara dayanarak soruyu yanıtla:\\n\\n\"\n",
    "        f\"Soru: {query}\\n\\n\"\n",
    "        \"Kaynaklar:\\n\"\n",
    "    )\n",
    "    prefix_ids = tokenizer(prefix, return_tensors=\"pt\").input_ids[0]\n",
    "    prefix_len = len(prefix_ids)\n",
    "\n",
    "    # 5) Kaynak metinlerini bütçeye göre ekle\n",
    "    budget = max_pos - max_new - prefix_len - reserved\n",
    "    included = []\n",
    "    total = 0\n",
    "    for s in sources:\n",
    "        tok = tokenizer(s, return_tensors=\"pt\").input_ids[0]\n",
    "        if total + len(tok) <= budget:\n",
    "            included.append(s)\n",
    "            total += len(tok)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    sources_text = \"\\n\\n\".join(included)\n",
    "\n",
    "    # 6) Tam prompt\n",
    "    prompt = prefix + sources_text + \"\\n\\nCevabı açıkla:\"\n",
    "\n",
    "    # 7) Üretim\n",
    "    output = text_generator(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    return output[0][\"generated_text\"]\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb1483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Örnek Kullanım ---\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"Metinde referans alınan kaynaklar nelerdir?\"\n",
    "    answer = rag_answer(question)\n",
    "    print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
